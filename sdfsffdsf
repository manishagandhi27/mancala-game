def flatten_document(doc):
    """
    Replace all $refs in the 'body' with the actual content from texts, tables, and pictures.
    Returns a list of content strings in the reading order.
    """
    # Build lookup dictionaries
    texts_lookup = { t["self_ref"]: t for t in doc.get("texts", []) }
    tables_lookup = { t["self_ref"]: t for t in doc.get("tables", []) }
    pictures_lookup = { p["self_ref"]: p for p in doc.get("pictures", []) } if "pictures" in doc else {}

    flat_content = []

    for el in doc.get("body", []):
        ref = el["$ref"]
        if ref.startswith("#/texts/") or ref.startswith("#/text/"):
            text_obj = texts_lookup.get(ref) or texts_lookup.get(ref.replace("#/text/", "#/texts/"))
            if text_obj:
                flat_content.append(text_obj.get("text", ""))
        elif ref.startswith("#/tables/"):
            table_obj = tables_lookup.get(ref)
            if table_obj:
                # Convert table cells to a simple markdown table (or any other human-readable format)
                md_table = convert_table_to_markdown(table_obj["data"]["table_cells"])
                flat_content.append(md_table)
        elif ref.startswith("#/picture/"):
            pic_obj = pictures_lookup.get(ref)
            if pic_obj:
                # Use caption or alt text if available, otherwise a placeholder.
                caption = pic_obj.get("caption", f"[Image {pic_obj['self_ref']}]")
                flat_content.append(caption)
        else:
            # If the element type is unknown, you might simply ignore or add a placeholder.
            flat_content.append(f"[Unknown element: {ref}]")
    return flat_content

def convert_table_to_markdown(table_cells):
    """
    Given a list of table cell dictionaries, convert them into a Markdown table.
    Assumes that cells have row and column indices. (You might need to customize this.)
    """
    # For simplicity, assume a two-row header + data rows layout.
    # Build a dictionary with (row, col) -> text.
    table_dict = {}
    max_row = 0
    max_col = 0
    for cell in table_cells:
        row = cell.get("start_row_offset_idx", 0)
        col = cell.get("start_col_offset_idx", 0)
        table_dict[(row, col)] = cell.get("text", "")
        max_row = max(max_row, row)
        max_col = max(max_col, col)
    
    # Create rows of text.
    rows = []
    for r in range(max_row+1):
        row_cells = [ table_dict.get((r, c), "") for c in range(max_col+1) ]
        rows.append("| " + " | ".join(row_cells) + " |")
    
    # Create a header separator if first row is header
    if rows:
        header_sep = "| " + " | ".join(["---"] * (max_col+1)) + " |"
        rows.insert(1, header_sep)
    return "\n".join(rows)

def chunk_text(flat_content, max_tokens=300, overlap=50):
    """
    Given a list of content strings (flattened), create chunks that maintain context.
    Here we simply join elements until we hit a token limit.
    (You can also use a proper tokenizer to count tokens.)
    """
    # For simplicity, we'll use whitespace-split words as a token approximation.
    chunks = []
    current_chunk = ""
    current_tokens = 0

    def count_tokens(text):
        return len(text.split())

    for content in flat_content:
        tokens = count_tokens(content)
        # If adding this content exceeds max_tokens, create a new chunk with overlap.
        if current_tokens + tokens > max_tokens:
            chunks.append(current_chunk.strip())
            # Use overlap from the end of the current chunk.
            current_chunk_words = current_chunk.split()
            overlap_words = " ".join(current_chunk_words[-overlap:]) if overlap < len(current_chunk_words) else current_chunk
            current_chunk = overlap_words + " " + content
            current_tokens = count_tokens(current_chunk)
        else:
            current_chunk += "\n" + content
            current_tokens += tokens
    if current_chunk.strip():
        chunks.append(current_chunk.strip())
    return chunks

# ===== Example Usage =====
if __name__ == "__main__":
    # Assume `doc` is your JSON after merging tables.
    flat_content = flatten_document(doc)
    chunks = chunk_text(flat_content, max_tokens=300, overlap=50)
    for idx, chunk in enumerate(chunks):
        print(f"--- Chunk {idx+1} ---")
        print(chunk)
        print("\n")
